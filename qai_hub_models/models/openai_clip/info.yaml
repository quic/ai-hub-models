name: OpenAI-Clip
# id must match with the model dir name in qai_hub_models
id: openai_clip
status: public
headline: Multi-modal foundational model for vision and language tasks like image/text
  similarity and for zero-shot image classification.
domain: Multimodal
description: Contrastive Language-Image Pre-Training (CLIP) uses a ViT like transformer
  to get visual features and a causal language model to get the text features. Both
  the text and visual features can then be used for a variety of zero-shot learning
  tasks.
use_case: Image Classification
tags:
  - foundation
research_paper: https://arxiv.org/abs/2103.00020
research_paper_title: Learning Transferable Visual Models From Natural Language Supervision
license: https://github.com/openai/CLIP/blob/main/LICENSE
deploy_license: https://qaihub-public-assets.s3.us-west-2.amazonaws.com/qai-hub-models/Qualcomm+AI+Hub+Proprietary+License.pdf
source_repo: https://github.com/openai/CLIP/
technical_details:
  Model checkpoint: ViT-B/16
  Image input resolution: 224x224
  Text context length: 77
  Number of parameters (CLIPTextEncoder): 76.0M
  Model size (CLIPTextEncoder): 290 MB
  Number of parameters (CLIPImageEncoder): 115M
  Model size (CLIPImageEncoder): 437 MB
applicable_scenarios:
  - Image Search
  - Content Moderation
  - Caption Creation
related_models: []
form_factors:
  - Phone
  - Tablet
has_static_banner: true
has_animated_banner: true
license_type: mit
deploy_license_type: AI Model Hub License
dataset: []
