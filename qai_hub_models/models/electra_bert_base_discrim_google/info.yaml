name: Electra-Bert-Base-Discrim-Google
id: electra_bert_base_discrim_google
status: public
headline: Language model for masked language modeling and general-purpose NLP tasks.
domain: Generative AI
description: ELECTRABERT is a lightweight BERT model designed for efficient self-supervised learning of language representations. It can be used for identify unnatural or artificially modified text and as a backbone for various NLP tasks.
use_case: Text Generation
tags:
- backbone
applicable_scenarios:
- Text Classification
- Sentiment Analysis
- Named Entity Recognition
related_models:
- albert_base_v2_hf
form_factors:
- Phone
- Tablet
- IoT
- XR
has_static_banner: true
has_animated_banner: false
dataset: []
technical_details:
  Model checkpoint: google/electra-base-discriminator
  Input resolution: 1x384
  Number of parameters: 109M
  Model size (float): 417 MB
license_type: apache-2.0
research_paper: https://arxiv.org/abs/2003.10555
research_paper_title: 'ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators'
source_repo: https://github.com/google-research/electra/
license: https://github.com/google-research/electra/blob/master/LICENSE
