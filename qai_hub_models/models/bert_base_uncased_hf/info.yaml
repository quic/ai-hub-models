name: Bert-Base-Uncased-Hf
id: bert_base_uncased_hf
status: public
headline: Language model for masked language modeling and general-purpose NLP tasks.
domain: Generative AI
description: Bert is a lightweight BERT model designed for efficient self-supervised learning of language representations. It can be used for masked language modeling and as a backbone for various NLP tasks.
use_case: Text Generation
tags:
- backbone
applicable_scenarios:
- Text Classification
- Sentiment Analysis
- Named Entity Recognition
related_models:
- albert_base_v2_hf
form_factors:
- Phone
- Tablet
- IoT
- XR
has_static_banner: true
has_animated_banner: true
dataset: []
technical_details:
  Model checkpoint: google-bert/bert-base-uncased
  Input resolution: 1x384
  Number of parameters: 110M
  Model size (float): 418 MB
license_type: apache-2.0
research_paper: https://arxiv.org/abs/1810.04805
research_paper_title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
source_repo: https://github.com/google-research/bert
license: https://github.com/google-research/bert/blob/master/LICENSE
