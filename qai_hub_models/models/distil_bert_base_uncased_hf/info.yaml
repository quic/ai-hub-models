name: Distil-Bert-Base-Uncased-Hf
id: distil_bert_base_uncased_hf
status: public
headline: Language model for masked language modeling and general-purpose NLP tasks.
domain: Generative AI
description: DistilBERT is a lightweight BERT model designed for efficient self-supervised learning of language representations. It can be used for masked language modeling and as a backbone for various NLP tasks.
use_case: Text Generation
tags:
- backbone
applicable_scenarios:
- Text Classification
- Sentiment Analysis
- Named Entity Recognition
related_models:
- albert_base_v2_hf
form_factors:
- Phone
- Tablet
- IoT
- XR
has_static_banner: true
has_animated_banner: false
dataset: []
technical_details:
  Model checkpoint: distil_bert_base_uncased_hf
  Input resolution: 1x384
  Number of parameters: 11.3M
  Model size (float): 43.3 MB
license_type: apache-2.0
research_paper: https://arxiv.org/abs/1910.01108
research_paper_title: 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter'
license: https://huggingface.co/distilbert/distilbert-base-uncased/blob/f8354bcfbd3068faf2c5149654881cb4214e931d/LICENSE
